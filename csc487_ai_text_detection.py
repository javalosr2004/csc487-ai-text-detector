# -*- coding: utf-8 -*-
"""CSC487-AI-Text-Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MxPAZublJKYbzvwB02qVwS0XSQByEBsW

# 1. Imports
"""

import copy
import math

import pandas as pd
from sklearn.model_selection import train_test_split

import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

"""# 2. Data"""

df = pd.read_csv("data/Training_Essay_Data.csv")
df.head()

X_full = df["text"]
y_full = df["generated"]

X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

class EssayDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=512):
      self.texts = texts
      self.labels = labels
      self.tokenizer = tokenizer
      self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
      text = self.texts[index]
      label = self.labels[index]

      tokenize_output = self.tokenizer(
          text,
          padding="max_length",
          truncation=True,
          max_length=self.max_len,
          return_tensors="pt"
      )

      return {
          "input_ids": tokenize_output["input_ids"].squeeze(0),
          "attention_mask": tokenize_output["attention_mask"].squeeze(0),
          "label": torch.tensor(label, dtype=torch.long)
      }

train_dataset = EssayDataset(X_train, y_train, tokenizer)
test_dataset = EssayDataset(X_test, y_test, tokenizer)
val_dataset   = EssayDataset(X_val, y_val, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)

"""# 3. Helper Functions"""

def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

def subsequent_mask(size):
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(
        torch.uint8
    )
    return subsequent_mask == 0

def construct_mask(attention_mask):
    mask = (attention_mask == 0)
    return mask.unsqueeze(1).unsqueeze(2)

def attention(query, key, value, mask=None, dropout=None):
    """Compute attention with query, key, and value vectors"""
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

"""# 4. AITextDetectionModel"""

class AITextDetectionModel(nn.Module):
    """Transform (only-encoder) architecture with classifier head"""
    def __init__(self, source_embedding, encoder_stack, classifier_head):
        super(AITextDetectionModel, self).__init__()
        self.source_embedding = source_embedding
        self.encoder_stack = encoder_stack
        self.classifier_head = classifier_head

    def forward(self, source, source_mask):
        embedded = self.source_embedding(source)
        encoded_output = self.encoder_stack(embedded, source_mask)
        cls_embedding = encoded_output[:, 0, :]
        logits = self.classifier_head(cls_embedding)
        return logits

"""## Part 1: Embedding and Positional Encoding Layers"""

class Embeddings(nn.Module):
    "Embedding Layer: converts tokens into vectors"
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.embedding = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
    "Postional Encoder: allows transformer to understand order of token embeddings"
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)
        return self.dropout(x)

"""## Part 2: Encoder Stack"""

class EncoderStack(nn.Module):
    def __init__(self, encoding_layer, N):
        "Encoder stack with N encoding layers"
        super(EncoderStack, self).__init__()
        self.encoding_layers = clones(encoding_layer, N)
        self.norm = LayerNorm(encoding_layer.size)

    def forward(self, x, mask):
        "Pass input and mask through each encoder layer"
        for layer in self.encoding_layers:
            x = layer(x, mask)
        return self.norm(x)

class EncoderLayer(nn.Module):
    def __init__(self, size, self_attention, feed_forward, dropout):
        "Encoder Layer consists of two sublayers: self-attention and feed forward layers"
        super(EncoderLayer, self).__init__()
        self.size = size
        self.self_attention = self_attention
        self.feed_forward = feed_forward
        self.sublayer1 = ResidualConnection(size, dropout)
        self.sublayer2 = ResidualConnection(size, dropout)

    def forward(self, x, mask):
        "Connect attention and feed forward sublayers"
        x = self.sublayer1(x, lambda x_: self.self_attention(x_, x_, x_, mask))
        return self.sublayer2(x, lambda x_: self.feed_forward(x_))

"""### Residual Connections and Layer Norm"""

class ResidualConnection(nn.Module):
    """Residual connection with a layer norm"""
    def __init__(self, size, dropout):
        super(ResidualConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to a sublayer (attention or feed-forward) with the same size."
        return x + self.dropout(sublayer(self.norm(x)))

class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

"""### Encoder Sublayer 1: Multi-Head Attention"""

class MultiHeadedAttention(nn.Module):
    "Multi-headed attention block inside encoder layer"
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h = h
        self.linear_layers = clones(nn.Linear(d_model, d_model), 4)
        self.attention = None
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # Linear projections in batch from d_model => h x d_k
        query, key, value = [
            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for lin, x in zip(self.linear_layers, (query, key, value))
        ]

        # Apply attention on all the projected vectors in batch
        x, self.attention = attention(
            query, key, value, mask=mask, dropout=self.dropout
        )

        # Concatenate batch and apply a final linear
        x = (
            x.transpose(1, 2)
            .contiguous()
            .view(nbatches, -1, self.h * self.d_k)
        )
        del query
        del key
        del value
        return self.linear_layers[-1](x)

"""### Encoder Sublayer 2: Positionwise FNN"""

class PositionwiseFeedForward(nn.Module):
    "Feed-forward network inside encoder layer"
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.dropout(self.activation(self.linear1(x)))
        return self.linear2(x)

"""## Classifier Head"""

class ClassifierHead(nn.Module):
    "Classify between human or AI-generated text"
    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, output_dim)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.dropout(self.activation(self.linear1(x)))
        x = self.sigmoid(self.linear2(x))
        return x

"""## Model Creation"""

def construct_model(vocab_size, N=6, d_model=512, d_ff=2048, h=8, num_classes=2, dropout=0.1):
    c = copy.deepcopy

    position_encoder = PositionalEncoding(d_model, dropout)
    source_embedding = nn.Sequential(Embeddings(d_model, vocab_size), c(position_encoder))

    attention_head = MultiHeadedAttention(h, d_model)
    feed_forward_network = PositionwiseFeedForward(d_model, d_ff, dropout)

    encoder_layer = EncoderLayer(d_model, c(attention_head), c(feed_forward_network), dropout)
    encoder_stack = EncoderStack(encoder_layer, N)

    classifier_head = ClassifierHead(input_dim=d_model, hidden_dim=d_model // 2, output_dim=num_classes, dropout=dropout)

    model = AITextDetectionModel(
        source_embedding,
        encoder_stack,
        classifier_head
    )

    # Initialize parameters with Glorot / fan_avg
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model

def inference_test():
    vocab_size = 1000
    model = construct_model(vocab_size, N=2, d_model=128, d_ff=512, h=4, num_classes=2, dropout=0.1)
    model.eval()
    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
    src_mask = torch.ones(1, 1, 10)

    batch_size = 1
    seq_len = 10
    source = torch.randint(0, vocab_size, (batch_size, seq_len))
    source_mask = torch.ones(batch_size, 1, seq_len)

    with torch.no_grad():
        logits = model(src, src_mask)
        probs = F.softmax(logits, dim=-1)
        predicted_class = torch.argmax(probs, dim=-1)

    print(f"Input shape: {src.shape}")
    print(f"Mask shape: {src_mask.shape}")
    print(f"Probabilities [Human, AI]: {probs}")
    print(f"Predicted class: {predicted_class.item()} (0 = Human-written, 1 = AI-generated)")


def run_tests(n_tests=10):
    for _ in range(n_tests):
        inference_test()

run_tests(10)