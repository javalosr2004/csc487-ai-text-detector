# -*- coding: utf-8 -*-
"""CSC487-AI-Text-Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MxPAZublJKYbzvwB02qVwS0XSQByEBsW

# 1. Imports
"""

import copy
import math
import os
from os.path import exists
import spacy
import time
import warnings

import altair as alt
import pandas as pd

import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.nn.functional import log_softmax, pad
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

"""# 2. Data"""

df = pd.read_csv("essay_data.csv")
df.head()
df.loc[:, "text"].str.len().mean()

"""# 3. Helper Functions"""

def show_example(fn, args=[]):
    if __name__ == "__main__":
        return fn(*args)

def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

def subsequent_mask(size):
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(
        torch.uint8
    )
    return subsequent_mask == 0

def attention(query, key, value, mask=None, dropout=None):
    """Compute attention with query, key, and value vectors"""
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

"""# 4. AITextDetectionModel"""

class AITextDetectionModel(nn.Module):
    """Transform (only-encoder) architecture"""
    def __init__(self, source_embedding, encoder_stack, classifier_head):
        super(AITextDetectionModel, self).__init__()
        self.source_embedding = source_embedding
        self.encoder_stack = encoder_stack
        self.classifier_head = classifier_head

    def forward(self, source, source_mask):
        encoded_output = self.encoder_stack(self.source_embedding(source), source_mask)
        output_embedding = encoded_output[:, 0, :]  # shape: (batch, hidden_dim)
        logits = self.classifier_head(output_embedding)
        return logits

"""## Part 1: Embedding and Positional Encoding Layers"""

class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.embedding = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)
        return self.dropout(x)

"""## Part 2: Encoder Stack"""

class EncoderStack(nn.Module):
    def __init__(self, encoding_layer, N):
        "Encoder stack with N encoding layers"
        super(EncoderStack, self).__init__()
        self.encoding_layers = clones(encoding_layer, N)
        self.norm = LayerNorm(encoding_layer.size)

    def forward(self, x, mask):
        "Pass the input and mask through each encoder layer"
        for layer in self.encoding_layers:
            x = layer(x, mask)
        return self.norm(x)

class EncoderLayer(nn.Module):
    def __init__(self, size, self_attention, feed_forward, dropout):
        "Encoder Layer consists of two sublayers: self-attention and feed forward layers"
        super(EncoderLayer, self).__init__()
        self.self_attention = self_attention
        self.feed_forward = feed_forward
        self.sublayer1 = ResidualConnection(size, dropout)
        self.sublayer2 = ResidualConnection(size, dropout)

    def forward(self, x, mask):
        "Connect "
        x = self.sublayer1 (x, lambda x: self.self_attention(x, x, x, mask))
        return self.sublayer2(x, self.feed_forward)

class ResidualConnection(nn.Module):
    """Residual connection with a layer norm"""
    def __init__(self, size, dropout):
        super(ResidualConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to a sublayer (attention or feed-forward) with the same size."
        return x + self.dropout(sublayer(self.norm(x)))

class LayerNorm(nn.Module):
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

"""## Encoder Sublayer 1: Multi-Head Attention"""

class MultiHeadedAttention(nn.Module):
    "Multi-headed attention block inside encoder layer"
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = [
            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for lin, x in zip(self.linears, (query, key, value))
        ]

        # 2) Apply attention on all the projected vectors in batch.
        x, self.attn = attention(
            query, key, value, mask=mask, dropout=self.dropout
        )

        # 3) "Concat" using a view and apply a final linear.
        x = (
            x.transpose(1, 2)
            .contiguous()
            .view(nbatches, -1, self.h * self.d_k)
        )
        del query
        del key
        del value
        return self.linears[-1](x)

"""## Encoder Sublayer 2: Positionwise FNN"""

class PositionwiseFeedForward(nn.Module):
    "Feed-forward network inside encoder layer"
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(self.w_1(x).relu()))

"""## Classifier Head"""

class ClassifierHead(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(0.1)
        self.linear2 = nn.Linear(hidden_dim, output_dim)
        self.activation = nn.GELU()

    def forward(self, x):
        x = self.dropout(self.activation(self.linear1(x)))
        x = self.linear2(x)
        return x

"""## Model Creation"""

def make_model(
    vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1
):
    "Construct AITextDetectionModel model from hyperparameters."
    c = copy.deepcopy
    attention_head = MultiHeadedAttention(h, d_model)
    feed_forward_network = PositionwiseFeedForward(d_model, d_ff, dropout)
    position_encoder = PositionalEncoding(d_model, dropout)

    model = AITextDetectionModel(
        nn.Sequential(Embeddings(d_model, vocab), c(position_encoder)),
        EncoderStack(EncoderLayer(d_model, c(attention_head), c(feed_forward_network), dropout), N),
        ClassifierHead()
    )

    # Initialize parameters with Glorot / fan_avg
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform_(p)
    return model

def inference_test():
    test_model = make_model(11, 11, 2)
    test_model.eval()
    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
    src_mask = torch.ones(1, 1, 10)

    memory = test_model.encode(src, src_mask)
    ys = torch.zeros(1, 1).type_as(src)

    for i in range(9):
        out = test_model.decode(
            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)
        )
        prob = test_model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        ys = torch.cat(
            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1
        )

    print("Example Untrained Model Prediction:", ys)


def run_tests():
    for _ in range(10):
        inference_test()


show_example(run_tests)

