# Model hyperparameters
model:
  d_model: 512
  d_ff: 2048
  h: 8
  N: 6
  dropout: 0.1

# Training settings
training:
  batch_size: 8          # Reduced from 32 to fit longer sequences in memory
  epochs: 10
  learning_rate: 0.0001
  max_seq_len: 2048      # Increased from 512 to capture more essay content
  seed: 42

# Paths
paths:
  data: "../data/Training_Essay_Data.csv"  # Updated to correct data location
  checkpoint_dir: "checkpoints_long"       # Separate checkpoint folder
  vocab: "vocab_long.json"                 # Separate vocab file

